{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u0CKIzY1C6-"
   },
   "source": [
    "# Transfer Learning & Self-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e73h-ubcgwZ"
   },
   "source": [
    "1.   The main objective of this notebook is to develop the semi-supervised learning task. For that purpose, first it is designed a model that starting from the ResNet network, builds the classifier that is used as the model in the Self-Training algorithm\n",
    "2.  Data generated is available in the data_stock_transfer_learning model\n",
    "3.  The model generated using the ResNet network was saved and it can also be found on that folder\n",
    "\n",
    "Thefore, if taking so, one can skip all the subsequent steps and go directly to the Self-Training section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAtoGF-9cgwb"
   },
   "source": [
    "Load necesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASzv7a2Ds2yo"
   },
   "outputs": [],
   "source": [
    "!pip install mpl_finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxcK2-Lc1C7G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from mpl_finance import candlestick2_ohlc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdQuXJZ8e5Zu"
   },
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGmWJE30cgwd"
   },
   "source": [
    "Load stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqdON_AWdLxm"
   },
   "outputs": [],
   "source": [
    "def load_data_stock():\n",
    "    '''\n",
    "    Loads stock data previously gathered from Yahoo Finance\n",
    "    # Args: None\n",
    "    # Returns: close, open, high, low and volume quotes\n",
    "    '''\n",
    "    # Load data\n",
    "    date = pd.read_csv('Data_stock_yahoo/date.csv',header=None)\n",
    "    aux = date.iloc[:,0].values\n",
    "    validSymbols = pd.read_csv('Data_stock_yahoo/selectedSymbols.csv', header=None)\n",
    "    validCols = validSymbols.iloc[0,:].values - 1\n",
    "    close_quotes = pd.read_csv('Data_stock_yahoo/close.csv', header=None, usecols=validCols)\n",
    "    open_quotes = pd.read_csv('Data_stock_yahoo/open.csv',header=None, usecols=validCols)\n",
    "    low_quotes = pd.read_csv('Data_stock_yahoo/low.csv',header=None, usecols=validCols)\n",
    "    high_quotes = pd.read_csv('Data_stock_yahoo/high.csv',header=None, usecols=validCols)\n",
    "    volume_quotes = pd.read_csv('Data_stock_yahoo/volume.csv',header=None, usecols=validCols)\n",
    "    # Rename df\n",
    "    # Col names --> stock ticker names\n",
    "    ticker = pd.read_csv('Data_stock_yahoo/ticker.csv', header=None)\n",
    "    valid_stock_tickers = ticker.loc[validCols, 0].values\n",
    "    close_quotes.columns = valid_stock_tickers\n",
    "    open_quotes.columns = valid_stock_tickers\n",
    "    high_quotes.columns = valid_stock_tickers\n",
    "    low_quotes.columns = valid_stock_tickers\n",
    "    volume_quotes.columns = valid_stock_tickers\n",
    "    # Row index --> date index\n",
    "    close_quotes.index = aux\n",
    "    open_quotes.index = aux\n",
    "    high_quotes.index = aux\n",
    "    low_quotes.index = aux\n",
    "    volume_quotes.index = aux\n",
    "    return close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjJdRg0Kcgwf"
   },
   "source": [
    "Download files that have already been labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMr1u3NOdLxm"
   },
   "outputs": [],
   "source": [
    "def download_files(): \n",
    "    '''\n",
    "    Loads the examples that have been already labeled using the Labeling notebook\n",
    "    # Args: None\n",
    "    # Return: data labeled using the Labeling notebook\n",
    "    '''\n",
    "    onlyfiles = [f for f in listdir('Data_labeled') if isfile(join('Data_labeled', f))]\n",
    "    data = []\n",
    "    for file_ in onlyfiles:\n",
    "        df = pd.read_csv(join('Data_labeled', file_), usecols=[1,2,3,4])    \n",
    "        data.append(df)\n",
    "\n",
    "    data = pd.concat(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "dBRDUDn1dLxn",
    "outputId": "4be473d7-7d03-449d-db1b-9649ce179a85"
   },
   "outputs": [],
   "source": [
    "data = download_files()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i9TCvyfdLxo"
   },
   "outputs": [],
   "source": [
    "close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes = load_data_stock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "id": "XGVwc1IAER9s",
    "outputId": "062e2f6a-aae4-41f7-ccc6-0535697aaf6e"
   },
   "outputs": [],
   "source": [
    "close_quotes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUihJO7-fKHf"
   },
   "source": [
    "Dataset creation: from the examples labeled, create a dataset in which each row is a new sample of 40 moving sliding window from a specific date and stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcK5Ac0udLxo"
   },
   "outputs": [],
   "source": [
    "def get_hist_window(hist, idx_date, idx_stock, window_size):\n",
    "    return hist.iloc[idx_date:idx_date+window_size, idx_stock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRaQGioAdLxo"
   },
   "outputs": [],
   "source": [
    "def get_df_sample(data, close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes, sample):\n",
    "    '''\n",
    "\n",
    "    # Args: \n",
    "      data: dataset created using the download_file function.\n",
    "      OHLC from Yahoo Finance\n",
    "      sample: number of example from the dataset to be analyzed.\n",
    "    # Returns:\n",
    "      df: a daframe that contains the OHCL on a 40 days window of a specific stock on a specific date \n",
    "    '''\n",
    "    window_size = 40\n",
    "    idx_stock = data.Ticker.iloc[sample]\n",
    "    idx_date = data.DateIndex.iloc[sample]\n",
    "    open_df = get_hist_window(open_quotes, idx_date, idx_stock, window_size)\n",
    "    close_df = get_hist_window(close_quotes, idx_date, idx_stock, window_size)\n",
    "    high_df = get_hist_window(high_quotes, idx_date, idx_stock, window_size)\n",
    "    low_df = get_hist_window(low_quotes, idx_date, idx_stock, window_size)\n",
    "    volume_df = get_hist_window(volume_quotes, idx_date, idx_stock, window_size)\n",
    "    df = pd.concat([open_df, close_df, high_df, low_df, volume_df], axis=1)\n",
    "    df.columns = ['open_price', 'close_price', 'high', 'low', 'volume']\n",
    "    df.index = pd.to_datetime(pd.to_datetime(df.index), format='%d-%m-%Y')\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHnki3gpsI1P"
   },
   "outputs": [],
   "source": [
    "df = get_df_sample(data, close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xCmocdh3sgCn",
    "outputId": "33b57d8f-e96a-4f0b-c537-0501c6542394"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk4Q22pugXOH"
   },
   "source": [
    "### FIGURES GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6QXLyk_cgwk"
   },
   "source": [
    "Generate figures to train the ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve42896bsPEn"
   },
   "outputs": [],
   "source": [
    "# Try one example\n",
    "from PIL import Image, ImageChops\n",
    "\n",
    "fig2save, ax = plt.subplots(figsize=(6,4))\n",
    "_ = candlestick2_ohlc(ax, df.open_price, df.high,\n",
    "                             df.low , df.close_price,\n",
    "                             colorup='g', colordown='r', width=0.66, alpha=1)\n",
    "_ = plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho-djTDagiHu"
   },
   "source": [
    "Generate the figures that have already been labeled and save them into separed folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FgRRSbsdLxp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    df = get_df_sample(data, close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes, i)\n",
    "    fig2save, ax = plt.subplots(figsize=(6,4))\n",
    "    _ = candlestick2_ohlc(ax, df.open_price, df.high,\n",
    "                                df.low , df.close_price,\n",
    "                                colorup='g', colordown='r', width=0.66, alpha=1)\n",
    "    _ = plt.axis('off')\n",
    "    if data.Label.iloc[i] == 1:\n",
    "        filename = \"Figures_labeled/True/ej_%i.jpg\"%i\n",
    "        fig2save.savefig(filename)\n",
    "    else:\n",
    "        filename = \"Figures_labeled/False/ej_%i.jpg\"%i\n",
    "        fig2save.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6y8wglYcgwo"
   },
   "source": [
    "Figure preparation: images center and trim, to help the classifier identify the patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1appdm6yJQi"
   },
   "outputs": [],
   "source": [
    "def trim(im):\n",
    "    '''\n",
    "    Trim an specific image\n",
    "    # Args: the imaged to be cropped\n",
    "    # Retunrs: the cropped image\n",
    "    '''\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        return im.crop(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faR31BsiyNa-"
   },
   "outputs": [],
   "source": [
    "def trimFiles(label): \n",
    "  '''\n",
    "  Trim all the images generated before\n",
    "  # Arg: the class label of the images to be cropped.\n",
    "  # Returns: None\n",
    "  '''\n",
    "  i = 0\n",
    "  if label == 1:\n",
    "    dir = 'True'\n",
    "    onlyfiles = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "    len(onlyfiles)\n",
    "    for file_ in onlyfiles:\n",
    "        image = Image.open(join(dir, file_))\n",
    "        image = trim(image)\n",
    "        filename = \"Figures_labeled/True_truncated/ej_%i.jpg\"%i\n",
    "        image.save(filename)\n",
    "        i += 1\n",
    "  else:\n",
    "    dir = 'False'\n",
    "    onlyfiles = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "    len(onlyfiles)\n",
    "    for file_ in onlyfiles:\n",
    "        image = Image.open(join(dir, file_))\n",
    "        image = trim(image)\n",
    "        filename = \"Figures_labeled/False_truncated/ej_%i.jpg\"%i\n",
    "        image.save(filename)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v624ojVDx7B7"
   },
   "outputs": [],
   "source": [
    "# Trim all files \n",
    "trimFiles(1)\n",
    "trimFiles(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atkkj6xidLxq"
   },
   "source": [
    "Convert images to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeX9ohI0dLxq",
    "outputId": "639ed5e9-d2d7-444f-d08a-9aabaf56cff1"
   },
   "outputs": [],
   "source": [
    "# Try one example to get dimensions\n",
    "onlyfiles = [f for f in listdir('Figures_labeled/True_truncated') if isfile(join('Figures_labeled/True_truncated', f))]\n",
    "dir = 'Figures_labeled/True_truncated/' + onlyfiles[0]\n",
    "image = Image.open(dir)\n",
    "image = image.resize((230, 230), Image.BILINEAR) \n",
    "image_array = np.array(image)\n",
    "sizes = image_array.shape\n",
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2CcWwj3hq-L"
   },
   "source": [
    "Images are converted into numpy array and before they are reshaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8n4VqdjdLxq"
   },
   "outputs": [],
   "source": [
    "def convert_images2array(dir): \n",
    "    '''\n",
    "    Converts the images into a np array and before they are reshaped to properly apply convolutions\n",
    "    # Args: directory of the folder\n",
    "    # Return: np array of the all the data joint\n",
    "    '''\n",
    "    onlyfiles = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "    len(onlyfiles)\n",
    "    data = []\n",
    "    for file_ in onlyfiles:\n",
    "        image = Image.open(join(dir, file_))\n",
    "        image = image.resize((230, 230), Image.BILINEAR) # square filters\n",
    "        image_array = np.array(image)\n",
    "        data.append(image_array)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewKwuCINdLxq"
   },
   "outputs": [],
   "source": [
    "x_true = convert_images2array('Figures_labeled/True_truncated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paNB-bLkdLxr"
   },
   "outputs": [],
   "source": [
    "x_false = convert_images2array('Figures_labeled/False_truncated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aN4nR4odLxr",
    "outputId": "50d21aa4-b119-45a8-ca05-d77cfd91c305"
   },
   "outputs": [],
   "source": [
    "# Add label\n",
    "y = np.zeros((len(x_true)+len(x_false),1))\n",
    "y[0:len(x_true)] = 1.\n",
    "X = np.vstack([x_true, x_false])\n",
    "print('X shape:',X.shape)\n",
    "print('y shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO0uuJaViSSm"
   },
   "source": [
    "### TRAIN RESNET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HrUTJRWdLxr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ivnFZxqjFpd"
   },
   "source": [
    "Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpy-X_YBdLxr"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y, stratify=y)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPg51PpzjSlV"
   },
   "source": [
    "Run the above cell if you want to get the data already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0No-3sndgOF"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qdy1KW2UdO2z"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "f = open('Data_stock_transfer_learning/y_train.pckl', 'rb')\n",
    "y_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_transfer_learning/y_test.pckl', 'rb')\n",
    "y_test = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_transfer_learning/x_train.pckl', 'rb')\n",
    "x_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_transfer_learning/x_test.pckl', 'rb')\n",
    "x_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-6d55IEdLxs"
   },
   "source": [
    "Transfer learning model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBxpKU3xdLxs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01pJSjrQdLxs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152 \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRqeO8GvdLxs",
    "outputId": "b5dca24f-410e-4d13-e64d-49b12bac8ac3"
   },
   "outputs": [],
   "source": [
    "sizes = x_train[0].shape\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G723ejkYdHyH"
   },
   "outputs": [],
   "source": [
    "base_model = ResNet152(weights='imagenet', include_top=False, input_shape=sizes, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycSv64zgdLxs"
   },
   "outputs": [],
   "source": [
    "base_model = ResNet152(weights='imagenet', include_top=False, input_shape=sizes, classes=2)\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1cT6q7edLxs"
   },
   "outputs": [],
   "source": [
    "# Build own classifier\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(10, activation='relu')(x)\n",
    "predictions = layers.Dense(2, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVe5FOltdLxt"
   },
   "outputs": [],
   "source": [
    "head_model = Model(inputs = base_model.input, outputs = predictions)\n",
    "head_model.compile(optimizer='adam', loss=losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0cOpoiekYJL"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEk0KcIedLxu"
   },
   "outputs": [],
   "source": [
    "history = head_model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udaNM1TxzUnd"
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "head_model.save('Data_stock_transfer_learning/transfer_learning_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIwh_ZgLFzjL"
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAo7L3h_kjV0"
   },
   "source": [
    " Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DW-yfiekya4"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "z1XuyCa8aVbh",
    "outputId": "9366ee3b-b664-4656-ae11-1f3e6cf19617"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_test_aux = pd.DataFrame(y_train).idxmax(axis=1).values\n",
    "cf_matrix = confusion_matrix(y_test_aux, np.argmax(y_pred, axis=1))\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "isGL1m5v_-N7",
    "outputId": "1d205f52-347c-4e8c-8d07-b8377a4e2cc8"
   },
   "outputs": [],
   "source": [
    "auc = metrics.roc_auc_score(y_test_aux, np.argmax(y_pred, axis=1))\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test_aux,  np.argmax(y_pred, axis=1))\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKMr0HL6z6pp"
   },
   "source": [
    "# Self-Training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltq40jOmk8L6"
   },
   "source": [
    "Import Transfer-Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lF4HBrktz6eB"
   },
   "outputs": [],
   "source": [
    " model = tf.keras.models.load_model('Data_stock_transfer_learning/transfer_learning_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im008Fqzk98t"
   },
   "source": [
    "Load data used in the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1qawySH0P4x"
   },
   "outputs": [],
   "source": [
    "f = open('Data_stock_transfer_learning/y_train.pckl', 'rb')\n",
    "y_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_transfer_learning/y_test.pckl', 'rb')\n",
    "y_test = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_transfer_learning/x_train.pckl', 'rb')\n",
    "x_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_transfer_learning/x_test.pckl', 'rb')\n",
    "x_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aytmg1o3lbeK"
   },
   "source": [
    "Generate data to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jp-U_aaP5yEI"
   },
   "outputs": [],
   "source": [
    "n_samples = 20000 \n",
    "data_unlabeled = pd.DataFrame(columns=['Ticker', 'DateIndex', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WF2qpz6n6M9p"
   },
   "outputs": [],
   "source": [
    "data_unlabeled.Ticker = np.random.randint(low=0, high=close_quotes.shape[1], size=(n_samples,))\n",
    "data_unlabeled.DateIndex = np.random.randint(low=0, high=close_quotes.shape[0], size=(n_samples,))\n",
    "data_unlabeled.Label = np.ones((n_samples,))*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gomNjl1Z6n_n",
    "outputId": "718a86f8-7d81-4fbc-c92a-41fb557c2025"
   },
   "outputs": [],
   "source": [
    "data_unlabeled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIIbEdjDcgwz"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6jQ3EPevu8x"
   },
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a Python Imaging Library ( PIL ) image\n",
    "    \"\"\"\n",
    "    # put the figure pixmap into a numpy array\n",
    "    buf = fig2data (fig)\n",
    "    w, h, d = buf.shape\n",
    "    return Image.frombytes(\"RGB\", (w ,h), buf.tostring())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23O2KxXhv2MG"
   },
   "outputs": [],
   "source": [
    "def fig2data(fig):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a numpy 3D array of RGBA values\n",
    "    \"\"\"\n",
    "    # draw the renderer\n",
    "    fig.canvas.draw ( )\n",
    " \n",
    "    # Get the RGBA buffer from the figure\n",
    "    w,h = fig.canvas.get_width_height()\n",
    "    buf = np.fromstring ( fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    buf.shape = (w, h, 3)\n",
    " \n",
    "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "    buf = np.roll (buf, 3, axis=2)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tabrzZX4nsys"
   },
   "source": [
    "For-loop to create the data to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gySH9IsmPbJ"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "validCols = np.zeros((data_unlabeled.shape[0]))\n",
    "for i in range(idx_start, data_unlabeled.shape[0]):\n",
    "    df = get_df_sample(data_unlabeled, close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes, i)\n",
    "    if (df.dropna().shape ==  df.shape): # there is no NaNs\n",
    "      fig2save, ax = plt.subplots(figsize=(6,4))\n",
    "      _ = candlestick2_ohlc(ax, df.open_price, df.high,\n",
    "                                  df.low , df.close_price,\n",
    "                                  colorup='g', colordown='r', width=0.66, alpha=1)\n",
    "      _ = plt.axis('off')\n",
    "      image = trim(fig2img(fig2save))\n",
    "      image = image.resize((230, 230), Image.BILINEAR) \n",
    "      image_array = np.array(image)\n",
    "      data.append(image_array)\n",
    "      validCols[i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96GkeK7CnKAw"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = data_unlabeled[validCols == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sx_bw4MC4Yd2"
   },
   "outputs": [],
   "source": [
    "x_unlabeled = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qmzn8BqX4Yd3"
   },
   "outputs": [],
   "source": [
    "# Add label\n",
    "y = np.ones((len(x_unlabeled),1))*(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ecF36XT9453"
   },
   "source": [
    "Algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBcYEca4-ddA"
   },
   "outputs": [],
   "source": [
    "# Copy structure\n",
    "x_train_semi = x_train\n",
    "x_test_semi = x_test\n",
    "y_train_semi = y_train\n",
    "y_test_semi = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGU6hAuVmAvd"
   },
   "outputs": [],
   "source": [
    "def re_train_model(x_train, y_train, x_test, y_test):\n",
    "  '''\n",
    "  This functions retrains the transfer learning model developed before\n",
    "  # Args: xtrain, ytrain, xtest, ytest\n",
    "  # Returns: None but the model has been trained\n",
    "  '''\n",
    "  model.compile(optimizer='adam', loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "  history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fv2IDUt97z9"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "while (len(x_unlabeled)>0):\n",
    "    print('Still remaining', len(x_unlabeled), 'samples to label...')\n",
    "    # 1:4 label in each iteration\n",
    "    n_pool_to_label = int((len(x_train_semi) + len(x_test_semi))/2)\n",
    "    # Saturation of unlabeled data in the last iterations\n",
    "    if(len(x_unlabeled) < n_pool_to_label):\n",
    "        n_pool_to_label = len(x_unlabeled) \n",
    "    # Probability of the model\n",
    "    y_pred = model.predict(x_unlabeled[:n_pool_to_label])\n",
    "    y_pred = to_categorical(np.argmax(y_pred, axis=1))\n",
    "    x_train_semi = np.vstack([x_train_semi, x_unlabeled[:n_pool_to_label]])\n",
    "    y_train_semi = np.vstack([y_train_semi, y_pred])\n",
    "    # Remove current pool from unlabeled data\n",
    "    x_unlabeled = x_unlabeled[n_pool_to_label:]\n",
    "    # Re train the model \n",
    "    re_train_model(x_train_semi, y_train_semi, x_test_semi, y_test_semi)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer_Self_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
