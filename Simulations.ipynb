{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGqSti3Iy8kT"
   },
   "source": [
    "# SIMULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-Df7hlKy_cJ"
   },
   "source": [
    "This notebook runs all the simulation created for analyze the results included in the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9DHcJp3aVbd"
   },
   "source": [
    "## MNIST SIMULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtlxhXbgspdm"
   },
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDZJjVnnaVbd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "!pip install fitter\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "# import tensorflow_probability as tfp\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from utils_mnist import FluidNetwork\n",
    "from utils_mnist import GA\n",
    "from utils_mnist import make_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUgQrpd5aVbd"
   },
   "outputs": [],
   "source": [
    "def load_data_mnist():\n",
    "  '''\n",
    "  Loads MNIST data\n",
    "  # Returns: MNIST data normalized\n",
    "  '''\n",
    "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "  x_train = x_train/255.\n",
    "  x_test = x_test/255.\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHAa7hxlGC3S"
   },
   "outputs": [],
   "source": [
    "def data_augmentation(x_train):\n",
    "  '''\n",
    "  Build ImageDataGenerator \n",
    "  # Args: x_train\n",
    "  # Returns: ImageDataGenerator instance fitted\n",
    "  '''\n",
    "  datagen = ImageDataGenerator(\n",
    "      featurewise_center=False,\n",
    "      featurewise_std_normalization=False,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      validation_split=0.2)\n",
    "  datagen.fit(x_train)\n",
    "  return datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKKqEO8ftR8o"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpc1JYqutXIG"
   },
   "source": [
    "Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auhQYf6uGC3S",
    "outputId": "4a3fa245-48a1-4cdb-d4ee-c58e8aaf8e82"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ldr72RLuHa6"
   },
   "outputs": [],
   "source": [
    "X = np.vstack((x_train, x_test))\n",
    "y = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obTt7vdUuWvU"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X[1:2000], y[1:2000], test_size=0.3, stratify=y[1:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsGJ11XjwmFg"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDfDytzcGC3S"
   },
   "outputs": [],
   "source": [
    "x_train_aug = np.reshape(x_train, (x_train.shape[0],x_train.shape[1], x_train.shape[2],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmQziWSZtbqn"
   },
   "source": [
    "Fit ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgLB4A8AGC3S"
   },
   "outputs": [],
   "source": [
    "datagen = data_augmentation(x_train_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzE2g-Y4tkfT"
   },
   "source": [
    "### Keras model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bwj0ghIhHr5P"
   },
   "outputs": [],
   "source": [
    "# Parameters selection\n",
    "input_dim = 28\n",
    "output_dim = 10\n",
    "num_max_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pzjLXg4aVbe"
   },
   "outputs": [],
   "source": [
    "def build_model(max_layers):\n",
    "  '''\n",
    "  Builds a sequential model defined by max_layers\n",
    "  # Args: max_layers of the model\n",
    "  # Returns: model \n",
    "  '''\n",
    "  inputs = keras.Input(shape=(input_dim, input_dim,1), name=\"digits\")\n",
    "\n",
    "  for layers in range(max_layers):\n",
    "    if layers == 0:  \n",
    "      x = keras.layers.Flatten()(inputs)\n",
    "    else:\n",
    "      x = keras.layers.Dense(num_max_units, activation=\"relu\")(x)\n",
    "\n",
    "  outputs = keras.layers.Dense(output_dim, activation=\"softmax\", name=\"classification\")(x)\n",
    "  return  keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhzSM1TIr75E"
   },
   "outputs": [],
   "source": [
    "model = build_model(4)\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  \n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPbn5DFRsAVF"
   },
   "outputs": [],
   "source": [
    "x_train_aug = np.reshape(x_train, (x_train.shape[0],x_train.shape[1], x_train.shape[2],1))\n",
    "x_test_aug = np.reshape(x_test, (x_test.shape[0],x_test.shape[1], x_test.shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qU6815tcMEae",
    "outputId": "298b9469-6cf9-438b-daab-16d35e0c567c"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPIugU-UuWU8"
   },
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ9xam_wNJJG"
   },
   "outputs": [],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(datagen.flow(x_train_aug, y_train, batch_size=128,\n",
    "         subset='training'),\n",
    "         validation_data =(x_test_aug, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7FhK6YZuX6m"
   },
   "source": [
    "Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHjKEyLkMUYH",
    "outputId": "239b0d0c-6f16-40eb-b34c-29376987c011"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = model.predict(x_test_aug)\n",
    "y_pred_ = pd.DataFrame(y_pred).idxmax(axis=1).values\n",
    "y_test_aux = pd.DataFrame(y_test).idxmax(axis=1).values\n",
    "cf_matrix = confusion_matrix(y_test_aux, y_pred_)\n",
    "print(pd.DataFrame(cf_matrix))\n",
    "print('Accuracy:',accuracy_score(y_test_aux, y_pred_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEhXOxADueeW"
   },
   "source": [
    "### FluidNet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP1Z6D6PaVbg"
   },
   "outputs": [],
   "source": [
    "# Topological parameters\n",
    "window_size = 40\n",
    "max_layers = 3+3 # con 5 capas al principio no funciona\n",
    "num_max_units = 128\n",
    "# input_dim = window_size\n",
    "input_dim = 28*28\n",
    "# output_dim = 2\n",
    "output_dim = 10\n",
    "\n",
    "layers = np.zeros(max_layers, dtype='uint32')\n",
    "for i in range(max_layers): \n",
    "  if i == 0:\n",
    "    layers[i] = input_dim\n",
    "  elif i == max_layers-1:\n",
    "    layers[i] = output_dim\n",
    "  else:\n",
    "    layers[i] = num_max_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_mnist import load_data_mnist\n",
    "(x_train, y_train), (x_test, y_test) = load_data_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-2ms7kXaVbh"
   },
   "outputs": [],
   "source": [
    "net = FluidNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBoVLhk_aVbh",
    "outputId": "66db5453-296f-48cc-82be-218ab139c968"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 256\n",
    "epochs = 6000\n",
    "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
    "lr = 1e-3\n",
    "trigger = 0.01 # set <0.1 to enable AG flag\n",
    "print('Steps per epoch', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjmT5mieaVbh",
    "outputId": "e2733799-98b5-4235-f497-b882d56bc7b1"
   },
   "outputs": [],
   "source": [
    "history = net.train(\n",
    "    x_train,y_train,\n",
    "    x_test, y_test,\n",
    "    epochs, datagen,\n",
    "    batch_size, lr, trigger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzBUMDyvuqNX"
   },
   "source": [
    "## PATTERN RECOGNITION SIMULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPLSR9gjuvoh"
   },
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O07qxDQkuqNY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "!pip install fitter\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "# import tensorflow_probability as tfp\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from utils_patterns import FluidNetwork\n",
    "from utils_patterns import GA\n",
    "from utils_mnist import make_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid data preparation steps run the following cell and ignore the data preparation ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load data\n",
    "f = open('Data_stock_net_model/y_train_stock.pckl', 'rb')\n",
    "y_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_net_model/y_test_stock.pckl', 'rb')\n",
    "y_test = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_net_model/x_train_stock.pckl', 'rb')\n",
    "x_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('Data_stock_net_model/x_test_stock.pckl', 'rb')\n",
    "x_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlh1i9iAux-W"
   },
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaUD6YwNuqNa"
   },
   "outputs": [],
   "source": [
    "def load_data_stock():\n",
    "  '''\n",
    "  Loads stock data gathered from Yahoo Finance\n",
    "  '''\n",
    "  # Load data\n",
    "  date = pd.read_csv('Data_stock_yahoo/date.csv',header=None)\n",
    "  aux = date.iloc[:,0].values\n",
    "  validSymbols = pd.read_csv('Data_stock_yahoo/selectedSymbols.csv', header=None)\n",
    "  validCols = validSymbols.iloc[0,:].values - 1\n",
    "  close_quotes = pd.read_csv('Data_stock_yahoo/close.csv', header=None, usecols=validCols)\n",
    "  open_quotes = pd.read_csv('Data_stock_yahoo/open.csv',header=None, usecols=validCols)\n",
    "  low_quotes = pd.read_csv('Data_stock_yahoo/low.csv',header=None, usecols=validCols)\n",
    "  high_quotes = pd.read_csv('Data_stock_yahoo/high.csv',header=None, usecols=validCols)\n",
    "  volume_quotes = pd.read_csv('Data_stock_yahoo/volume.csv',header=None, usecols=validCols)\n",
    "  # Rename df\n",
    "  # Col names --> stock ticker names\n",
    "  ticker = pd.read_csv('Data_stock_yahoo/ticker.csv', header=None)\n",
    "  valid_stock_tickers = ticker.loc[validCols, 0].values\n",
    "  close_quotes.columns = valid_stock_tickers\n",
    "  open_quotes.columns = valid_stock_tickers\n",
    "  high_quotes.columns = valid_stock_tickers\n",
    "  low_quotes.columns = valid_stock_tickers\n",
    "  volume_quotes.columns = valid_stock_tickers\n",
    "  # Row index --> date index\n",
    "  close_quotes.index = aux\n",
    "  open_quotes.index = aux\n",
    "  high_quotes.index = aux\n",
    "  low_quotes.index = aux\n",
    "  volume_quotes.index = aux\n",
    "  return close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6cuPUb2uqNb"
   },
   "outputs": [],
   "source": [
    "close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes = load_data_stock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADzEqHHjuqNb"
   },
   "outputs": [],
   "source": [
    "def normalize_data(close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes):\n",
    "  '''\n",
    "  Normalize stock data using logaritmic returns\n",
    "  # Args: stock data\n",
    "  # Returns: stock data normalized\n",
    "  '''\n",
    "  close_quotes_pu = np.log(close_quotes).diff().fillna(0)\n",
    "  open_quotes_pu = np.log(open_quotes).diff().fillna(0)\n",
    "  high_quotes_pu = np.log(high_quotes).diff().fillna(0)\n",
    "  low_quotes_pu = np.log(low_quotes).diff().fillna(0)\n",
    "  volume_quotes_pu = np.log(volume_quotes).diff().fillna(0)\n",
    "  return close_quotes_pu, open_quotes_pu, high_quotes_pu, low_quotes_pu, volume_quotes_pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32LbrQ3PuqNc"
   },
   "outputs": [],
   "source": [
    "close_quotes_pu, open_quotes_pu, high_quotes_pu, low_quotes_pu, volume_quotes_pu = normalize_data(close_quotes, open_quotes, high_quotes, low_quotes, volume_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH0L65CmuqNc"
   },
   "outputs": [],
   "source": [
    "def download_files(): \n",
    "  ''' \n",
    "  Donwload labaled data from file\n",
    "  '''\n",
    "  onlyfiles = [f for f in listdir('Data_labeled') if isfile(join('Data_labeled', f))]\n",
    "\n",
    "  data = []\n",
    "\n",
    "  for file_ in onlyfiles:\n",
    "      df = pd.read_csv(join('Data_labeled', file_), usecols=[1,2,3,4])    \n",
    "      data.append(df)\n",
    "\n",
    "  data = pd.concat(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExLSYrkcuqNd"
   },
   "outputs": [],
   "source": [
    "def get_master_table(close_quotes_pu):\n",
    "  window_size = 40\n",
    "  master_table_aux = download_files()\n",
    "  master_table = pd.DataFrame(np.zeros((master_table_aux.shape[0], window_size+1)))\n",
    "  for i in range(master_table_aux.shape[0]):\n",
    "    init_date = master_table_aux.iloc[i, 1]\n",
    "    stock = master_table_aux.iloc[i, 2]\n",
    "    master_table.iloc[i, 0:window_size] = close_quotes_pu.iloc[init_date:init_date+window_size, stock].values\n",
    "  master_table.iloc[:, -1] = master_table_aux.iloc[:, -1].values\n",
    "  return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-QFRORVuqNd"
   },
   "outputs": [],
   "source": [
    "master_table = get_master_table(close_quotes_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amRzxkEGuqNe"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(master_table.iloc[:,:-1].values, master_table.iloc[:,-1].values, test_size=0.3, stratify=master_table.iloc[:,-1].values)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEGi8j9SvAK-"
   },
   "source": [
    "### Keras model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "num_max_units = 128\n",
    "max_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG2-Jm7yuqNe"
   },
   "outputs": [],
   "source": [
    "def build_model(max_layers):\n",
    "  inputs = tf.keras.Input(shape=(input_dim, ), name=\"patterns\")\n",
    "\n",
    "  for layers in range(max_layers):\n",
    "    if layers == 0:  \n",
    "      x = keras.layers.Dense(num_max_units, activation=\"relu\")(inputs)\n",
    "    else:\n",
    "      x = keras.layers.Dense(num_max_units, activation=\"relu\")(x)\n",
    "\n",
    "  outputs = keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification\")(x)\n",
    "  return  keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ts3Nr56uqNf"
   },
   "outputs": [],
   "source": [
    "model = build_model(max_layers)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  \n",
    "              metrics=tf.keras.metrics.Precision(0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEdi-Q43uqNf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, np.argmax(y_train, axis=1)[None].T, epochs=2, validation_data=(x_test, np.argmax(y_test, axis=1)[None].T), batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZIULSAiuqNg"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred[y_pred <= 0.5] = 0.\n",
    "y_pred[y_pred > 0.5] = 1.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFEqJEXBuqNg"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmTWcZu0vHvc"
   },
   "source": [
    "### FluidNet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meljz_lMuqNh"
   },
   "outputs": [],
   "source": [
    "# Topological parameters\n",
    "window_size = 40\n",
    "max_layers = 3+3 \n",
    "num_max_units = 128\n",
    "input_dim = window_size\n",
    "output_dim = 2\n",
    "\n",
    "layers = np.zeros(max_layers, dtype='uint32')\n",
    "for i in range(max_layers): \n",
    "  if i == 0:\n",
    "    layers[i] = input_dim\n",
    "  elif i == max_layers-1:\n",
    "    layers[i] = output_dim\n",
    "  else:\n",
    "    layers[i] = num_max_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZTVWyYRuqNh"
   },
   "outputs": [],
   "source": [
    "net = FluidNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2w5tz3BuqNi",
    "outputId": "66db5453-296f-48cc-82be-218ab139c968"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128\n",
    "epochs = 500\n",
    "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
    "lr = 1e-3\n",
    "trigger = 0.01 # set <0.1 to enable AG flag\n",
    "print('Steps per epoch', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmDrGnMTuqNi"
   },
   "outputs": [],
   "source": [
    "history = net.train(\n",
    "    x_train,y_train,\n",
    "    x_test, y_test,\n",
    "    epochs, steps_per_epoch,\n",
    "    batch_size, lr, trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_Y6IkjHuqNj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = net.predict(x_test)\n",
    "y_test_aux = pd.DataFrame(y_test).idxmax(axis=1).values\n",
    "cf_matrix = confusion_matrix(y_test_aux, y_pred)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ibfXzEjaD5V"
   },
   "source": [
    "### SMOTE for unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbPeTqpEaD5V"
   },
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mY42El1oaD5V"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gAXu9CrBYb1"
   },
   "outputs": [],
   "source": [
    "# Define pipeline strategy\n",
    "over = SMOTE(sampling_strategy=0.2)\n",
    "under = RandomUnderSampler(sampling_strategy=0.3)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzP-AtvCaD5V"
   },
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "x_train_os, y_train_os = pipeline.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ushwx5RaaD5V"
   },
   "outputs": [],
   "source": [
    "print(Counter(y_train_os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_train_os, y_train_os= shuffle(x_train_os, y_train_os,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY1zE2x_vM5c"
   },
   "source": [
    "Keras model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "num_max_units = 128\n",
    "max_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_layers):\n",
    "  inputs = tf.keras.Input(shape=(input_dim, ), name=\"patterns\")\n",
    "\n",
    "  for layers in range(max_layers):\n",
    "    if layers == 0:  \n",
    "      x = keras.layers.Dense(num_max_units, activation=\"relu\")(inputs)\n",
    "    else:\n",
    "      x = keras.layers.Dense(num_max_units, activation=\"relu\")(x)\n",
    "\n",
    "  outputs = keras.layers.Dense(1, activation=\"sigmoid\", name=\"classification\")(x)\n",
    "  return  keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_layers)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  \n",
    "              metrics=tf.keras.metrics.Precision(0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQJRzhY5vzQJ"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train_os, y_train_os, epochs=100, validation_data=(x_test, np.argmax(y_test, axis=1)[None].T), batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2b8Qs7kvzQJ"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred[y_pred <= 0.5] = 0.\n",
    "y_pred[y_pred > 0.5] = 1.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUujChf_vzQK"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmsHyaXXvPZr"
   },
   "source": [
    "FluidNet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGMw7MrCvoDn"
   },
   "outputs": [],
   "source": [
    "# Topological parameters\n",
    "window_size = 40\n",
    "max_layers = 3+3 \n",
    "num_max_units = 128\n",
    "input_dim = window_size\n",
    "output_dim = 2\n",
    "\n",
    "layers = np.zeros(max_layers, dtype='uint32')\n",
    "for i in range(max_layers): \n",
    "  if i == 0:\n",
    "    layers[i] = input_dim\n",
    "  elif i == max_layers-1:\n",
    "    layers[i] = output_dim\n",
    "  else:\n",
    "    layers[i] = num_max_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHCCx2AtvoDo"
   },
   "outputs": [],
   "source": [
    "net = FluidNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDutLX7OvoDo",
    "outputId": "66db5453-296f-48cc-82be-218ab139c968"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128\n",
    "epochs = 6000\n",
    "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
    "lr = 1e-3\n",
    "trigger = 0.01\n",
    "print('Steps per epoch', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIPWdEtnvtyL"
   },
   "outputs": [],
   "source": [
    "y_train_os_one_hot = to_categorical(y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4N0hPCOaD5W"
   },
   "outputs": [],
   "source": [
    "history = net.train(\n",
    "    x_train_os,y_train_os_one_hot,\n",
    "    x_test, y_test,\n",
    "    1000, steps_per_epoch,\n",
    "    batch_size, lr, trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCGz7DfPaD5W"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = net.predict(x_test)\n",
    "y_test_aux = pd.DataFrame(y_test).idxmax(axis=1).values\n",
    "cf_matrix = confusion_matrix(y_test_aux, y_pred)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ5gsth2wwst"
   },
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tEKTzffwxV9"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['val_acc'])\n",
    "\n",
    "plt.title('Accuracy para el MNIST con una red evolutiva')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.legend(['Train loss', 'Val loss'])\n",
    "\n",
    "plt.title('Loss para el MNIST con una red evolutiva')\n",
    "\n",
    "# Convergence check\n",
    "plt.figure()\n",
    "plt.plot(history['train_loss'][5500:6000])\n",
    "plt.plot(history['val_loss'][5500:6000])\n",
    "plt.legend(['Train loss', 'Val loss'])\n",
    "\n",
    "plt.title('Comprobación convergencia del loss para el MNIST con una red evolutiva')\n",
    "\n",
    "# Trigger analysis\n",
    "aux = np.diff(np.log(np.array(history['val_loss'])))\n",
    "plt.figure()\n",
    "plt.plot(aux[5:])\n",
    "plt.title('Diferencia del loss en cada epoch')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "O9DHcJp3aVbd",
    "KzBUMDyvuqNX"
   ],
   "name": "Simulations.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
